{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EtysfbrQIFr"
   },
   "source": [
    "# Machine Learning Applications for Health (COMP90089_2024_SM2)\n",
    "# Tutorial: DATA Wrangling\n",
    "\n",
    "### TOPICS:\n",
    "\n",
    "*   Missing Values\n",
    "*   Outliers\n",
    "*   Standardisation\n",
    "\n",
    "\n",
    "### DATA SET:\n",
    "\n",
    "* Extracted from MIMIC-II-demo: ICU stay details with comorbity and admission information \n",
    "\n",
    "\n",
    "### LIBRARIES:\n",
    "\n",
    "* pandas\n",
    "* numpy\n",
    "* seaborn\n",
    "* matplotlib\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Context: \n",
    "\n",
    "The MIMIC II demo clinical database, consists of data integrated from diferent information systems in the hospital and contains diverse information such as: patient demographics, medications, results of lab tests and more. It is a much earlier and smaller version of the data that is now contained in MIMIC-IV, which will be the primary focus of this subject. I've extracted some data from this database and saved it as cSV files which will be read in with Pandas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cE83vSyniti"
   },
   "source": [
    "## Import Libraries and uncomment the 'pip install' rows if any prior installation is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ys8oFVhP7Qp"
   },
   "outputs": [],
   "source": [
    "#!pip install seaborn\n",
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Display maximum columns on the screen\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FG9vT72sEBvr"
   },
   "source": [
    "#### Let's look at some comorbidity data\n",
    "\n",
    "For reference, this data was originally obtained using the following query:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM mimic2.comorbidity_scores\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKQ3kpVMCqKh",
    "outputId": "78fa0410-68d3-4d57-81ed-7e4fd5575763"
   },
   "outputs": [],
   "source": [
    "\n",
    "comorbidity = pd.read_csv(\"https://raw.githubusercontent.com/melbourne-cdth/comp90089_tutorial2/main/comorbidity.csv\")\n",
    "#Print the output and the for the available column names\n",
    "\n",
    "print(comorbidity.columns)\n",
    "comorbidity.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ut3JQYvzHNDL"
   },
   "source": [
    "### Richer data set\n",
    "\n",
    "Now that we have an idea of how the table looks like, let's gather more information from parts of MIMIC to make the data richer in details.\n",
    "\n",
    "* **First table** to consider: icustay_detail (rename it icu to make it shorter)\n",
    "\n",
    "We will then join information from **2 other Tables** based on 1 mutual criteria.\n",
    "* Second Table: comorbidity_scores\n",
    "* Third Table: admissions\n",
    "\n",
    "**Criteria:** The Hospital admission id must be the same (column hadm_id). This is an integer number identifying an ICU stay.\n",
    "\n",
    "**Note** that we are now specifying the columns we want to retrieve from each table, except for the comorbidity_score that we want to retrieve all columns.\n",
    "\n",
    "The query used to get this data was as follows:\n",
    "\n",
    "```sql\n",
    "SELECT comorb.*, adm.admit_dt, adm.disch_dt, icu.gender, icu.dod, icu.expire_flg, icu.hospital_los, icu.icustay_los, icu.sapsi_first, icu.sapsi_max, icu.sofa_first, icu.sofa_max\n",
    "FROM mimic2.icustay_detail as icu\n",
    "INNER JOIN mimic2.comorbidity_scores as comorb \n",
    "ON comorb.hadm_id = icu.hadm_id\n",
    "INNER JOIN mimic2.admissions as adm\n",
    "ON adm.hadm_id = icu.hadm_id\n",
    "ORDER BY subject_id\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtYnzhFMHNaR",
    "outputId": "ea5ad03b-6f06-42ad-9a9c-c4703393aaa8"
   },
   "outputs": [],
   "source": [
    "\n",
    "result_merged = pd.read_csv(\"https://raw.githubusercontent.com/melbourne-cdth/comp90089_tutorial2/main/comorbidity_v2.csv\")\n",
    "#Print the output and the for the available column names\n",
    "print(result_merged.columns)\n",
    "result_merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LW23fRprmkFq"
   },
   "source": [
    "## **Type of data: Numerical or Categorical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1goQiptvmahN",
    "outputId": "8be96445-d6af-4448-f37a-063b3ebfac4c"
   },
   "outputs": [],
   "source": [
    "## Check the type of data(Numerical or Categorical)\n",
    "## Dtype in this set: float64, int64, object\n",
    "\n",
    "result_merged.info()\n",
    "\n",
    "categ_columns = [col for col in result_merged.columns if result_merged[col].dtype=='object']\n",
    "numerical_columns = [col for col in result_merged.columns if (result_merged[col].dtype=='float64' or result_merged[col].dtype=='int64')]\n",
    "\n",
    "print(\"\\n Categorical features in this data set: \",categ_columns,\"\\n\")\n",
    "print(\"\\n Numerical features in this data set: \",numerical_columns,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "DGZGkdzclJxi",
    "outputId": "9cf3d0b2-f5e4-4cc2-fe6d-9c3fc8771cea"
   },
   "outputs": [],
   "source": [
    "## View some statistical measurements of this data (only for numerical columns):\n",
    "\n",
    "result_merged.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhH5H-jPhyNI"
   },
   "source": [
    "## Checking for Missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJimugnWrcAy",
    "outputId": "25468bf7-a4cb-4b7c-a400-5117ee4f8ddc"
   },
   "outputs": [],
   "source": [
    "##Missing values\n",
    "result_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9Isix5st9Ex"
   },
   "source": [
    "## Checking for Outliers\n",
    "\n",
    "* Outliers are observations that deviate significantly from other values in a dataset.\n",
    "* There are many ways to classify a numerical value as an outlier, here we will see one based on the Interquartile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTgmKH_sjQuu"
   },
   "outputs": [],
   "source": [
    "# Function to Detection Outlier on one-dimentional datasets.\n",
    "# IQR based filtering  (Interquartile Range)\n",
    "\n",
    "def find_outliers(df, col_name):   \n",
    "    percentile25 = result_merged[col_name].quantile(0.25)\n",
    "    percentile75 = result_merged[col_name].quantile(0.75)\n",
    "    \n",
    "    iqr = percentile75-percentile25\n",
    "    upper_limit = percentile75 + 1.5 * iqr\n",
    "    lower_limit = percentile25 - 1.5 * iqr\n",
    "    \n",
    "    anomalies_upper = result_merged[col_name][result_merged[col_name] > upper_limit]\n",
    "    anomalies_lower = result_merged[col_name][result_merged[col_name] < lower_limit]\n",
    "    return_file_up = pd.DataFrame(anomalies_upper)\n",
    "    return_file_lw = pd.DataFrame(anomalies_lower)\n",
    "\n",
    "    if anomalies_upper.size != 0 and anomalies_lower.size != 0:\n",
    "      anomalies_frame = pd.concat([return_file_up,return_file_lw])\n",
    "\n",
    "    elif anomalies_lower.size != 0:\n",
    "      anomalies_frame = return_file_lw\n",
    "\n",
    "    elif anomalies_upper.size != 0:\n",
    "      anomalies_frame = return_file_up\n",
    "      \n",
    "    return anomalies_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "dwSfc3rgu5X9",
    "outputId": "c1882e0b-df59-4493-9f71-45b143c2161b"
   },
   "outputs": [],
   "source": [
    "## Length of stay in ICU (icustay_los) Column Outliers based on IQR Filtering:\n",
    "find_outliers(result_merged,'icustay_los')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJVhM9pr9aR-"
   },
   "source": [
    "## Replacing values\n",
    "\n",
    "* One of the ways to deal with missing values or outliers is to remove the values with too many missing variables using a threshold.\n",
    "* Or to replace it with some statistical measurement (mean, median, quantile...)\n",
    "\n",
    "But in all cases you should evaluate if the strategy will make sense to the data or not. Outliers can also be rare information underlying the data and no action will be require on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SOfNnJo9aR_",
    "outputId": "3bd0b9ac-6dc7-45fe-f485-3c9f9a4cb729"
   },
   "outputs": [],
   "source": [
    "## Numerical Columns with missing values: sapsi_first,sapsi_max,sofa_first,sofa_max\n",
    "\n",
    "## Let's see how to replace missing values for sapsi_first by the Quartile 50.\n",
    "\n",
    "quart50_sapsi_first = result_merged['sapsi_first'].quantile(0.5)\n",
    "print(\"Value of Quantile 50% for sapsi_first column: \",quart50_sapsi_first,\"\\n\")\n",
    "\n",
    "print(\"Rows with NaN values in sapsi_first column: \\n\",result_merged['sapsi_first'][result_merged['sapsi_first'].isna()],\"\\n\")\n",
    "\n",
    "# Replace the NaN values with the Quartile 50. Check them again.\n",
    "result_merged['sapsi_first'].fillna(quart50_sapsi_first, inplace=True)\n",
    "print(\"Rows with NaN values in sapsi_first column: \",result_merged['sapsi_first'][result_merged['sapsi_first'].isna()],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc2gwt2X9aR_"
   },
   "source": [
    "## Transforming Categorical into Numbers:\n",
    "\n",
    "We will see in the next Tutorials that, when dealing with Machine Learning, we ideally transform categorical data into Numbers.\n",
    "\n",
    "* Categorical columns in this data set:  ['category', 'admit_dt', 'disch_dt', 'gender', 'dod', 'expire_flg'] \n",
    "* The method **get_dummies** from pandas library can convert the columns with object or category dtype into numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwNvn4TC9aSA",
    "outputId": "e3df651e-7f76-4609-fe71-e4cda053d320"
   },
   "outputs": [],
   "source": [
    "# Transform the Column Gender using get_dummies. It will create 2 new columns with binary values for Male or Female.\n",
    "\n",
    "gender_categorical = pd.get_dummies(result_merged['gender'])\n",
    "print(gender_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aZ0IpuJ9aSA"
   },
   "source": [
    "## Compute pairwise correlation of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqVIYL9B9aSA",
    "outputId": "27f28664-f7c8-4afb-d3fa-075651dadbf1"
   },
   "outputs": [],
   "source": [
    "#Exclude unecessary columns first\n",
    "result_merged_clean = result_merged.drop(['subject_id','category', 'gender', 'expire_flg','hadm_id','admit_dt','disch_dt','dod'], axis = 1)\n",
    "\n",
    "#Create the correlation variable. The .coor() method has some default parameters.\n",
    "correlation = result_merged_clean.corr() ##default method='pearson'\n",
    "\n",
    "#Print the correlation output values\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JYq9rgpJUb0S",
    "outputId": "419f25cc-6ffb-48c3-c76e-9fc448d259d3"
   },
   "outputs": [],
   "source": [
    "# Plot the output figure\n",
    "plt.figure(figsize= (20,20))\n",
    "sns.heatmap(correlation, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
